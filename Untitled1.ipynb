{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0282985b-1117-4d65-b8ee-bc0e971f1301",
   "metadata": {},
   "source": [
    "---\n",
    "## 包依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185f9e3d-7ec9-4a0d-87b4-f2f92c498c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 包依赖\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1197b-a915-4d7b-901c-950ff2295ba4",
   "metadata": {},
   "source": [
    "---\n",
    "## 使用pytorch建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc679e9-0d03-4e67-a03b-e94e82d06f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN+LSTM+自互注意力模型\n",
    "#   Classification Model\n",
    "\n",
    "# params：\n",
    "# feature_size                                 输入特征个数 21个\n",
    "# temporal_size                                时间步长度  101个时间步\n",
    "# cnn_kernel_size                              卷积核长度\n",
    "# cnn_kernel_num                               卷积核个数\n",
    "# lstm_layer                                   LSTM层个数\n",
    "# self_att_hide                                自注意力层神经元数量\n",
    "# n                                            相同时间内，选取前n个分数最高的特征；相同特征下，选取前n个分数最高的时间\n",
    "# m                                            选取对单一特征，单一时间片段下，影响最大的前m个任意时间片段下的任意特征\n",
    "\n",
    "class CnnLstmModel(nn.Module):\n",
    "    def __init__(self, feature_size, temporal_size, cnn_kernel_size, cnn_kernel_num, lstm_layer, self_att_dim, inter_att_dim, n, m):\n",
    "        super(CnnLstmModel, self).__init__()\n",
    "        \n",
    "        # init param\n",
    "        self.feature_size = feature_size\n",
    "        self.temporal_size = temporal_size\n",
    "        self.cnn_kernel_size = cnn_kernel_size\n",
    "        self.cnn_kernel_num = cnn_kernel_num\n",
    "        self.lstm_layer = lstm_layer\n",
    "        self.self_att_dim = self_att_dim\n",
    "        self.inter_att_dim = inter_att_dim\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.dim = self.n * (self.feature_size + self.temporal_size)\n",
    "        \n",
    "        # CNN-layer:\n",
    "        # 输入:[batch_size, feature_size, temporal_size]\n",
    "        # 输出:[batch_size, feature_size * cnn_kernel_num, temporal_size]\n",
    "        self.cnnin = self.feature_size\n",
    "        self.cnnout = self.feature_size * self.cnn_kernel_num\n",
    "        self.cnn_padding = (self.cnn_kernel_size - 1) / 2\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels= self.cnnin, \n",
    "            out_channels= self.cnnout, \n",
    "            kernel_size= self.cnn_kernel_size, \n",
    "            stride= 1, \n",
    "            padding = self.cnn_padding\n",
    "        )\n",
    "        \n",
    "        # LSTM-layer:\n",
    "        # 输入:[feature_size, temporal_size， cnn_kernel_num] 需要转置操作\n",
    "        # 输出 = 输入。需要设置 hidden_size = cnn_kernel_num\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size= self.cnn_kernel_num,\n",
    "            hidden_size = self.cnn_kernel_num,\n",
    "            num_layers = self.lstm_layer,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        #Add&Norm\n",
    "\n",
    "        # Self-Attention:\n",
    "        # 输入:[batch_size, feature_size, temporal_size, cnn_kernel_num]\n",
    "        # 输出:[batch_size, feature_size, temporal_size, 1]\n",
    "        self.self_att_score = 1\n",
    "        self.self_att = nn.Sequential(\n",
    "            nn.Linear(self.cnn_kernel_num, self.self_att_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.self_att_dim, self.self_att_score)\n",
    "        )\n",
    "        \n",
    "        # Inter-Attention\n",
    "        # 输入: \n",
    "        #     1.CNN+LSTM的输出结果: [batch_size, feature_size, temporal_size, cnn_kernel_num]\n",
    "        #     2.选择后的Self-Attention的输出结果: [batch_size, (feature_size + temporal_size) * n, 1, cnn_kernel_num]\n",
    "        # 输出: [batch_size, (feature_size + temporal_size)*n*(m+1), 1, cnn_kernel_num]\n",
    "        # K:\n",
    "        # K的输入:[batch_size, feature_size, temporal_size, cnn_kernel_num]\n",
    "        # K的输出:[batch_size, feature_size, temporal_size, inter_att_dim]\n",
    "        self.k = nn.Sequential(\n",
    "            nn.Linear(self.cnn_kernel_num, self.inter_att_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.inter_att_dim, self.inter_att_dim)\n",
    "        )\n",
    "        # Q:\n",
    "        # Q的输入:[batch_size, (feature_size + temporal_size) * n, 1, cnn_kernel_num]\n",
    "        # Q的输出:[batch_size, (feature_size + temporal_size) * n, 1, inter_att_dim]\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Linear(self.cnn_kernel_num, self.inter_att_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.inter_att_dim, self.inter_att_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "        # 2dconv-layer\n",
    "        # 输入:[batch_size, n*(temporal_size+ feature_size), (m+1), cnn_kernel_num]\n",
    "        # 输出:[batch_size, class_size]\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # 第一层卷积:保持不变\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.dim,        # 输入通道数\n",
    "                out_channels=self.dim,       # 输出通道数\n",
    "                kernel_size=(3, 3),          # 卷积核大小\n",
    "                padding=1                    # 填充以保持空间维度\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),  # 池化减少空间维度\n",
    "            \n",
    "            # 第二层卷积:缓慢增加\n",
    "            nn.Conv2d(self.dim, self.dim * 2, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(self.dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            \n",
    "            # 第三层卷积\n",
    "            nn.Conv2d(self.dim * 2, 256, kernel_size=(3, 3), padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # 全局平均池化\n",
    "        )\n",
    "        # [batch_size, 256, 1, 1]\n",
    "        \n",
    "        # 全连接分类层\n",
    "        self.fc = nn.Linear(256, num_classes = 3) #设置分类类别为三类 (<1.6, >1.6且<1.9, >1.9)\n",
    "        \n",
    "        #softmax\n",
    "        self.softmax = nn.softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x:[batch_size, temporal_size, feature_size]\n",
    "        ### cnn layer\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # x:[batch_size, feature_size, temporal_size]\n",
    "        \n",
    "        x = self.cnn(x)\n",
    "        # x:[batch_size, feature_size * cnn_kernel_num, temporal_size]\n",
    "        \n",
    "        x = x.view(x.size(0), self.feature_size, self.cnn_kernel_num, x.size(2))\n",
    "        # x:[batch_size, feature_size, cnn_kernel_num, temporal_size]\n",
    "        \n",
    "        cnn_out = x.permute(0, 1, 3, 2)\n",
    "        # cnn_out:[batch_size, feature_size, temporal_size, cnn_kernel_num]\n",
    "\n",
    "\n",
    "        \n",
    "        ### lstm layer\n",
    "        x = cnn_out.view(cnn_out.size(0) * cnn_out.size(1), cnn_out.size(2), cnn_out.size(3))\n",
    "        # x:[batch_size * feature_size, temporal_size, cnn_kernel_num]\n",
    "        \n",
    "        x = self.lstm(x)\n",
    "        # x:[batch_size * feature_size, temporal_size, cnn_kernel_num]\n",
    "        \n",
    "        lstm_out = x.view(self.batch_size, self.feature_size, x.size(1), x.size(2))\n",
    "        # lstm_out:[batch_size, feature_size, temporal_size, cnn_kernel_num]\n",
    "\n",
    "\n",
    "        \n",
    "        ###cnn和lstm层结果的归一化  \n",
    "        union_out = cnn_out + lstm_out\n",
    "\n",
    "\n",
    "        \n",
    "        ### self-att layer\n",
    "        x = union_out.view(-1, self.cnn_kernel_num)\n",
    "        # x:[batch_size * feature_size * temporal_size, cnn_kernel_num]\n",
    "        \n",
    "        x = self.self_att(x)\n",
    "        # x:[batch_size * feature_size * temporal_size, 1]\n",
    "        \n",
    "        score = x.view(self.batch_size, self.feature_size, self.temporal_size)\n",
    "        # score:[batch_size, feature_size, temporal_size]\n",
    "\n",
    "\n",
    "\n",
    "        ## 同一时间维度选取得分最高的前n个特征\n",
    "        x = score.permute(0, 2, 1)\n",
    "        # x:[batch_size, temporal_size, feature_size]\n",
    "\n",
    "        temporal_topn_score, temporal_topn_indices = torch.topk(x, self.n, dim = 2)\n",
    "        # temporal_topn_score, temporal_topn_indices:[batch_size, temporal_size, n]\n",
    "\n",
    "        temporal_expanded_indices = temporal_topn_indices.unsqueeze(-1).expand(-1, -1, -1, self.cnn_kernel_num)\n",
    "        # temporal_expanded_indices:[batch_size, temporal_size, n, cnn_kernel_num]\n",
    "\n",
    "        temporal_topn_values = torch.gather(union_out, 2, temporal_expanded_indices)\n",
    "        # temporal_topn_values:[batch_size, temporal_size, n, cnn_kernel_num]\n",
    "        \n",
    "        ## 同一特征维度选取得分最高的前n个时间\n",
    "        feature_topn_score, feature_topn_indices = torch.topk(score, self.n, dim = 2)\n",
    "        # feature_topn_score, feature_topn_indices:[batch_size, feature_size, n]\n",
    "\n",
    "        feature_expanded_indices = feature_topn_indices.unsqueeze(-1).expand(-1, -1, -1, self.cnn_kernel_num)\n",
    "        # feature_expanded_indices:[batch_size, feature_size, n, cnn_kernel_num]\n",
    "\n",
    "        feature_topn_values = torch.gather(union_out, 2, feature_expanded_indices)\n",
    "        # feature_topn_values:[batch_size, feature_size, n, cnn_kernel_num]\n",
    "\n",
    "\n",
    "        ### inter-att layer\n",
    "        #待定\n",
    "        topn_values = []\n",
    "        # topn_values:[batch_size, (feature_size + temporal_size) * n, cnn_kernel_num]\n",
    "\n",
    "        ## \n",
    "        x = union_out.view(union_out.size(0) * union_out.size(1) * union_out.size(2), union_out.size(3))\n",
    "        # x:[batch_size * feature_size * temporal_size, cnn_kernel_num]\n",
    "\n",
    "        x = self.k(x)\n",
    "        # x:[batch_size * feature_size * temporal_size, inter_att_dim]\n",
    "        k = x.view(self.batch_size, self.feature_size, self.temporal_size, -1)\n",
    "        # k:[batch_size, feature_size, temporal_size, inter_att_dim]\n",
    "\n",
    "        x = topn_values.view(topn_values.size(0) * topn_values.size(1), topn_values.size(2))\n",
    "        # x:[batch_size * (feature_size + temporal_size) * n, cnn_kernel_num]\n",
    "        \n",
    "        x = self.q(x)\n",
    "        # x:[batch_size * (feature_size + temporal_size) * n, inter_att_dim]\n",
    "        \n",
    "        q = x.view(self.batch_size, -1, x.size(1))\n",
    "        # q:[batch_size, (feature_size + temporal_size) * n, inter_att_dim]\n",
    "        \n",
    "        inter_att_score = torch.einsum('bftd,bqd->bqft', k, q)\n",
    "        # inter_att_score:[batch_size, (feature_size + temporal_size) * n, feature_size, temporal_size]\n",
    "\n",
    "        #初始化结果张量\n",
    "        dim1 = (self.feature_size + self.temporal_size) * self.n\n",
    "        inter_att_result = torch.zeros(self.batch_size, dim1, self.m, self.cnn_kernel_num)\n",
    "        for b in range(self.batch_size):\n",
    "            for d in range(dim1):\n",
    "                # 获取当前注意力分数矩阵 [feature_size, temporal_size]\n",
    "                att_matrix = inter_att_score[b, d]\n",
    "\n",
    "                # 展平注意力矩阵并获取前m个最大值的索引\n",
    "                flat_att = att_matrix.view(-1)\n",
    "                topk_values, topk_indices = torch.topk(flat_att, m, dim=0)\n",
    "\n",
    "                # 将扁平索引转换为二维索引 (feature_idx, temporal_idx)\n",
    "                feature_indices = topk_indices // temporal_size\n",
    "                temporal_indices = topk_indices % temporal_size\n",
    "                \n",
    "                # 从union_out中提取对应的特征\n",
    "                for i, (f_idx, t_idx) in enumerate(zip(feature_indices, temporal_indices)):\n",
    "                    result[b, d, i] = union_out[b, f_idx, t_idx]\n",
    "        # inter_att_result = [batch_size, (feature_size + temporal_size) * n, m, cnn_kernel]\n",
    "\n",
    "        topn_values_expanded = topn_values.unsqueeze(2)\n",
    "        # [batch_size, dim1, cnn_kernel_num] -> [batch_size, dim1, 1, cnn_kernel_num]\n",
    "\n",
    "        combined = torch.cat([topn_values_expanded, inter_att_result], dim=2)\n",
    "        # combined = [batch_size, dim1, m+1, cnn_kernel_num]\n",
    "        conv2d_out = self.conv_layer(combined)\n",
    "        conv2d_out = conv2d_out.view(conv2d_out.size(0), -1)\n",
    "        fc_out = self.fc(conv2d_out)\n",
    "        # fc_out = [batch_size, num_classes]\n",
    "        out = self.softmax(fc_out)\n",
    "\n",
    "        return out\n",
    "        # cnn-layer:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4ed0d-273e-416c-937b-40707dd66fb7",
   "metadata": {},
   "source": [
    "---\n",
    "## 训练前的参数与数据配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff831473-bba4-4925-90e7-9ace23d358be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据X\n",
    "folder_30s = r\"../lib/attention_model_data/30s\"\n",
    "folder_28s = r\"../lib/attention_model_data/28s\"\n",
    "folder_26s = r\"../lib/attention_model_data/26s\"\n",
    "\n",
    "X_30s = []\n",
    "X_28s = []\n",
    "X_26s = []\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_30s) \n",
    "             if f.endswith('.csv') and os.path.isfile(os.path.join(folder_30s, f))]\n",
    "for f in csv_files:\n",
    "    #合成数据文件路径\n",
    "    file_x_30s = os.path.join(folder_30s, f)\n",
    "    file_x_28s = os.path.join(folder_28s, f)\n",
    "    file_x_26s = os.path.join(folder_26s, f)\n",
    "    \n",
    "    #读取数据\n",
    "    df_30s = pd.read_csv(file_x_30s)\n",
    "    df_28s = pd.read_csv(file_x_28s)\n",
    "    df_26s = pd.read_csv(file_x_26s)\n",
    "\n",
    "    #将数据从dataframe格式转换为torch支持的tensor结构\n",
    "    np_x_30s = df_30s.to_numpy()\n",
    "    np_x_28s = df_28s.to_numpy()\n",
    "    np_x_26s = df_26s.to_numpy()\n",
    "\n",
    "    np_x_30s = np_x_30s.astype(np.float32)\n",
    "    np_x_28s = np_x_28s.astype(np.float32)\n",
    "    np_x_26s = np_x_26s.astype(np.float32)\n",
    "\n",
    "    torch_x_30s = torch.tensor(np_x_30s)\n",
    "    torch_x_28s = torch.tensor(np_x_28s)\n",
    "    torch_x_26s = torch.tensor(np_x_26s)\n",
    "\n",
    "    X_30s.append(torch_x_30s)\n",
    "    X_28s.append(torch_x_28s)\n",
    "    X_26s.append(torch_x_26s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab9c902-6b9a-4609-bdee-0c7cd66fd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据y\n",
    "file_target_y_value = r\"../lib/target_y/attention_model_y_value.csv\"\n",
    "file_target_y_label = r\"../lib/target_y/attention_model_y_label.csv\"\n",
    "Y_value = pd.read_csv(file_target_y_value)\n",
    "Y_label = pd.read_csv(file_target_y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33beafb8-7698-4e00-b9c8-de3fd1cc755e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of stacked_x: torch.Size([2895, 30, 21])\n",
      "shape of changed X: torch.Size([2895, 21, 30])\n",
      "size of feachers: 21\n",
      "shape of normed X: torch.Size([2895, 30, 21])\n"
     ]
    }
   ],
   "source": [
    "#使用batchnorm归一化X\n",
    "#将X从list类型转为tensor\n",
    "stacked_X_30s = torch.stack(X_30s)\n",
    "stacked_X_28s = torch.stack(X_28s)\n",
    "stacked_X_26s = torch.stack(X_26s)\n",
    "\n",
    "\n",
    "#对数据集使用层归一化\n",
    "print(\"shape of stacked_x:\", stacked_X_30s.shape)\n",
    "stacked_X_30s = stacked_X_30s.permute(0,2,1)\n",
    "stacked_X_28s = stacked_X_28s.permute(0,2,1)\n",
    "stacked_X_26s = stacked_X_26s.permute(0,2,1)\n",
    "print(\"shape of changed X:\", stacked_X_30s.shape)\n",
    "print(\"size of feachers:\", stacked_X_30s.size(1))\n",
    "\n",
    "\n",
    "# 创建batchnorm类对象\n",
    "batchnorm = nn.BatchNorm1d(stacked_X_30s.size(1))\n",
    "\n",
    "\n",
    "# 对三个数据集采用batchnorm归一化\n",
    "temp_normed_X = batchnorm(stacked_X_30s)\n",
    "normed_X_30s = temp_normed_X.permute(0,2,1)\n",
    "\n",
    "temp_normed_X = batchnorm(stacked_X_28s)\n",
    "normed_X_28s = temp_normed_X.permute(0,2,1)\n",
    "\n",
    "temp_normed_X = batchnorm(stacked_X_26s)\n",
    "normed_X_26s = temp_normed_X.permute(0,2,1)\n",
    "\n",
    "print(\"shape of normed X:\", normed_X_30s.shape)\n",
    "\n",
    "\n",
    "#清除梯度 消除grad_fn参数\n",
    "normed_X_30s = normed_X_30s.detach()\n",
    "normed_X_28s = normed_X_28s.detach()\n",
    "normed_X_26s = normed_X_26s.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518983b9-76e3-40f1-9d83-1127a4f07c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将y处理为模型能接受的tensor格式\n",
    "# Y = Y.iloc[:,1]\n",
    "# tensor_Y_value    Y的取值 值为g值\n",
    "# tensor_Y_label    Y的标签 值为[1,2,3]\n",
    "np_Y_value = Y_value.to_numpy()\n",
    "np_Y_value = np_Y_value.astype(np.float32)\n",
    "tensor_Y_value = torch.tensor(np_Y_value)\n",
    "\n",
    "np_Y_label = Y_label.to_numpy()\n",
    "np_Y_label = np_Y_label.astype(np.float32)\n",
    "tensor_Y_label = torch.tensor(np_Y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846722b8-dcc1-48a9-bef0-c96001539e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2895, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tensor_Y_value.shape)\n",
    "print(tensor_Y_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa10b9-9092-49ac-9f86-a97de0102ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和测试集\n",
    "# normed_X_30s    30s数据集\n",
    "# normed_X_28s    28s数据集\n",
    "# normed_X_26s    26s数据集\n",
    "# tensor_Y        标签Y值\n",
    "\n",
    "\n",
    "# 构建dataset\n",
    "# 采用取值构建模型-->回归模型\n",
    "dataset_30s = data_utils.TensorDataset(normed_X_30s, tensor_Y_value)\n",
    "dataset_28s = data_utils.TensorDataset(normed_X_28s, tensor_Y_value)\n",
    "dataset_26s = data_utils.TensorDataset(normed_X_26s, tensor_Y_value)\n",
    "\n",
    "train_size = int(len(dataset)*2/3)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "# split dataset\n",
    "train_dataset_30s, test_dataset_30s = data_utils.random_split(\n",
    "    dataset_30s,\n",
    "    [train_size, test_size]\n",
    ")\n",
    "train_dataset_28s, test_dataset_28s = data_utils.random_split(\n",
    "    dataset_28s,\n",
    "    [train_size, test_size]\n",
    ")\n",
    "train_dataset_26s, test_dataset_26s = data_utils.random_split(\n",
    "    dataset_26s,\n",
    "    [train_size, test_size]\n",
    ")\n",
    "print(f\"训练集大小: {len(train_dataset_28s)}\")\n",
    "print(f\"测试集大小: {len(test_dataset_26s)}\")\n",
    "\n",
    "\n",
    "# 创建训练集和测试集的dataloader\n",
    "# 设置:训练集中batchsize = 10，测试集中batchsize = 5\n",
    "train_dataloader_30s = data_utils.DataLoader(train_dataset_30s, batch_size=10, shuffle=True)\n",
    "test_dataloader_30s = data_utils.DataLoader(test_dataset_30s, batch_size=5, shuffle=True)\n",
    "\n",
    "train_dataloader_28s = data_utils.DataLoader(train_dataset_28s, batch_size=10, shuffle=True)\n",
    "test_dataloader_28s = data_utils.DataLoader(test_dataset_28s, batch_size=5, shuffle=True)\n",
    "\n",
    "train_dataloader_26s = data_utils.DataLoader(train_dataset_26s, batch_size=10, shuffle=True)\n",
    "test_dataloader_26s = data_utils.DataLoader(test_dataset_26s, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8bfa5-6b07-4b04-a9c8-5b1922d28c0c",
   "metadata": {},
   "source": [
    "---\n",
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d957df1-d467-4c87-98f1-e45b32ac4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型参数设置\n",
    "# train_dataloader_30s    30s训练集\n",
    "# train_dataloader_28s    28s训练集\n",
    "# train_dataloader_26s    26s训练集\n",
    "# test_dataloader_30s     30s训练集\n",
    "# test_dataloader_28s     28s训练集\n",
    "# test_dataloader_26s     26s训练集\n",
    "# device = \"cpu\" or \"cuda\"\n",
    "\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 0.01\n",
    "\n",
    "#迭代次数\n",
    "epoch = 400\n",
    "\n",
    "# 设置cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 使用GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # 使用CPU\n",
    "print(\"device:\", device)\n",
    "\n",
    "# 定义模型超参数\n",
    "# class CnnLstmModel(nn.Module):\n",
    "#     def __init__(self, feature_size, temporal_size, cnn_kernel_size, cnn_kernel_num, lstm_layer, self_att_dim, inter_att_dim, n, m):\n",
    "feature_size = stacked_X_30s.size(1)\n",
    "temporal_size_30s = stacked_X_30s.size(2)\n",
    "temporal_size_28s = stacked_X_28s.size(2)\n",
    "temporal_size_26s = stacked_X_26s.size(2)\n",
    "cnn_kernel_size = 5\n",
    "cnn_kernel_num = 10\n",
    "lstm_layer = 1\n",
    "self_att_dim = 8\n",
    "inter_att_dim = 8\n",
    "n = 3\n",
    "m = 5\n",
    "\n",
    "# 定义模型\n",
    "model_30s = CnnLstmModel(\n",
    "    feature_size = feature_size,\n",
    "    temporal_size = temporal_size_30s,\n",
    "    cnn_kernel_size = cnn_kernel_size,\n",
    "    cnn_kernel_num = cnn_kernel_num,\n",
    "    lstm_layer = lstm_layer,\n",
    "    self_att_dim = self_att_dim,\n",
    "    inter_att_dim = inter_att_dim,\n",
    "    n = n,\n",
    "    m = m\n",
    ").to(device)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion_MSE = nn.MSELoss()    #MSE 处理回归任务\n",
    "criterion_CrossEntropy = nn.CrossEntropyLoss()    #交叉熵损失函数\n",
    "\n",
    "\n",
    "# 定义迭代器\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256cd387-fb14-4188-8695-f935ef0f3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ac65409-f913-49af-b28a-17adc1aa090a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader_30s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, label \u001b[38;5;129;01min\u001b[39;00m train_dataloader_30s:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(inputs))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader_30s' is not defined"
     ]
    }
   ],
   "source": [
    "for inputs, label in train_dataloader_30s:\n",
    "    print(type(inputs))\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59243a47-e66c-42ad-b042-592ef5f3fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练 30s数据集\n",
    "model_30s.train()\n",
    "loss_store = []\n",
    "for i in range(epoch):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_30s:\n",
    "\n",
    "        no_grad_inputs = inputs.detach()#再次消除输出中的grad_fn参数 防止意外\n",
    "\n",
    "        #使用gpu加速\n",
    "        gpu_inputs = no_grad_inputs.to(device)\n",
    "        gpu_labels = labels.to(device)\n",
    "        \n",
    "        outputs = model_30s(gpu_inputs)#模型输出 .squeeze(-1)\n",
    "        loss = criterion_MSE(outputs,gpu_labels)#计算损失\n",
    "        optimizer.zero_grad()#清零梯度\n",
    "        loss.backward()#反向传播\n",
    "        optimizer.step()#更新参数\n",
    "\n",
    "        total_loss += loss.item()#统计损失\n",
    "        \n",
    "    # 计算一次epoch的平均损失\n",
    "    ave_loss = total_loss / len(train_dataset_30s):.4f\n",
    "    #打印训练过程中的损失\n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f'Epoch [{i + 1}/{epoch}], Loss: {ave_loss}')\n",
    "    # 储存损失值\n",
    "    loss_store.append(ave_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054284f9-e0f8-4f48-a63a-8ab9ce6b7936",
   "metadata": {},
   "source": [
    "---\n",
    "### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d3e59-bb64-456c-9d83-bc084b5f8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试 30s\n",
    "model_30s.eval()\n",
    "eval_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader_30s:\n",
    "        outputs = model_30s(inputs.to(device))  # .squeeze(-1)\n",
    "        loss = criterion_MSE(outputs,labels.float().to(device))\n",
    "        eval_loss += loss.item()\n",
    "    print(f'eval_loss: {eval_loss / len(test_dataset):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d44aac-2daf-4f11-af45-cf4e9dd138ac",
   "metadata": {},
   "source": [
    "---\n",
    "### 结束训练后的保存配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d29a31-30c5-45e9-ab8b-fde5c0c86105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型配置\n",
    "torch.save(model_30s.state_dict(), r'model_weights.pth') \n",
    "\n",
    "# # 加载时，需要先重新创建模型结构\n",
    "# model = MyModelClass(*args, **kwargs)  # 必须与原始模型结构相同\n",
    "# model.load_state_dict(torch.load('model_weights.pth'))\n",
    "# model.eval()  # 重要：将模型设置为评估模式（关闭dropout等训练特定层）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
